import json, sys, os
from pathlib import Path
from graph_based_rag_long import GraphRAG
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import tiktoken

# Change working directory to project root
PROJECT_ROOT = Path(__file__).parent.parent
os.chdir(PROJECT_ROOT)

# Initialize encoder
enc = tiktoken.encoding_for_model("gpt-4o")

# Configuration
input_path       = "UltraDomain/Mix/qa.json"
output_path      = "Result/Ours/mix_result.json"
chunk_log_path   = "Result/Ours/Chunks/used_chunks_mix.jsonl"
temp_output_path = output_path.replace(".json", "_temp.json")

MAX_WORKERS = 30
TOP_K1 = 50
TOP_K2 = 5

# Create result directories
os.makedirs(os.path.dirname(output_path), exist_ok=True)
os.makedirs(os.path.dirname(chunk_log_path), exist_ok=True)

# Create GraphRAG instance
rag = GraphRAG()
chunk_log_file = open(chunk_log_path, "w", encoding="utf-8")

# Load questions
with open(input_path, 'r', encoding='utf-8') as f:
    questions = json.load(f)

# Initialize result list
output_data = [None] * len(questions)

# Processing function
def process(index_query):
    idx, item = index_query
    query = item.get("query", "")
    try:
        answer, spent, context = rag.answer(query=query, top_k1=TOP_K1, top_k2=TOP_K2)
        chunk_ids = getattr(rag, "last_chunk_ids", [])
    except Exception as e:
        answer = f"[Error] {e}"
        spent = 0.0
        context = ""
        chunk_ids = []

    # chunk-id log
    for cid in chunk_ids:
        chunk_log_file.write(json.dumps({"query": query, "chunk_id": cid}, ensure_ascii=False) + "\n")

    # Sentence-based chunk-id log
    sentence_chunk_ids = set(getattr(rag, "all_sentence_chunk_ids", []))
    for cid in sentence_chunk_ids:
        chunk_log_file.write(json.dumps({"query": query, "sentence_chunk_id": cid}, ensure_ascii=False) + "\n")

    # ê²°ê³¼ ì €ì¥
    result = {
        "query": query,
        "result": answer,
        "time": spent,
        "context_token": context,
    }
    return idx, result

# ë³‘ë ¬ ì‹¤í–‰
completed = 0
save_every = 10

with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    futures = [executor.submit(process, (i, item)) for i, item in enumerate(questions)]
    for future in tqdm(as_completed(futures), total=len(futures), desc="Generating answers"):
        idx, result = future.result()
        output_data[idx] = result
        completed += 1

        if completed % save_every == 0:
            with open(temp_output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)

chunk_log_file.close()

# ìµœì¢… ê²°ê³¼ ì €ì¥
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, indent=2, ensure_ascii=False)

print(f"âœ… ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {output_path}")

# # í‰ê·  ì‹œê°„ ë° í† í° ìˆ˜ ê³„ì‚°
# valid_items = [it for it in output_data if it and not it["result"].startswith("[Error]")]

# if valid_items:
#     avg_time = sum(it["time"] for it in valid_items) / len(valid_items)
#     avg_tokens = sum(len(enc.encode(it["context_token"])) for it in valid_items) / len(valid_items)

#     print(f"\nğŸ“Š í‰ê·  ì†Œìš” ì‹œê°„: {avg_time:.2f}ì´ˆ")
#     print(f"ğŸ“Š í‰ê·  ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜: {avg_tokens:.1f}ê°œ")
# else:
#     print("âš ï¸ ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ì–´ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
