import json
from graph_based_rag_chunks import GraphRAG
# from graph_based_rag_chunks import GraphRAG  # Import the updated class
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

# ì…ë ¥/ì¶œë ¥ ê²½ë¡œ
input_path = "UltraDomain/qa.json"
output_path = "UltraDomain/result/kgrag_original.json"
temp_output_path = output_path.replace(".json", "_temp.json")

# (1) ê²°ê³¼ ë””ë ‰í„°ë¦¬ ì—†ìœ¼ë©´ ë§Œë“¤ê¸° â”€ ê°€ì¥ ë¨¼ì €!
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# GraphRAG ì¸ìŠ¤í„´ìŠ¤
rag = GraphRAG()

# ì…ë ¥ ë¡œë”©
with open(input_path, 'r', encoding='utf-8') as f:
    questions = json.load(f)

# ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸ (index ìˆœì„œ ë³´ì¡´)
output_data = [None] * len(questions)

# ì‘ì—… í•¨ìˆ˜
def process(index_query):
    idx, item = index_query
    query = item.get("query", "")
    try:
        answer, spent, tokens = rag.answer(query)
        print(answer)
    except Exception as e:
        answer = f"[Error] {e}"
        spent  = 0.0
        tokens = 0
    
    spent  = float(spent)
    tokens = int(tokens)
    
    result = {"query": query, "result": answer, "time": spent, "context_token": tokens}
    return idx, result

# ë³‘ë ¬ ì²˜ë¦¬
completed = 0
save_every = 10

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(process, (i, item)) for i, item in enumerate(questions)]
    for future in tqdm(as_completed(futures), total=len(futures), desc="Generating answers"):
        idx, result = future.result()
        output_data[idx] = result  # ìˆœì„œ ìœ ì§€
        completed += 1

        # 10ê°œë§ˆë‹¤ ì„ì‹œ ì €ì¥
        if completed % save_every == 0:
            with open(temp_output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            # print(f"[Temp Save] {completed} items saved to {temp_output_path}")

# í‰ê·  ì†Œìš” ì‹œê°„ ë° í† í° ìˆ˜ ê³„ì‚°
valid_items = [it for it in output_data if it and not it["result"].startswith("[Error]")]

if valid_items:
    avg_time   = sum(it["time"]        for it in valid_items) / len(valid_items)
    avg_tokens = sum(it["context_token"] for it in valid_items) / len(valid_items)

    print(f"\nğŸ“Š í‰ê·  ì†Œìš” ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"ğŸ“Š í‰ê·  ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜: {avg_tokens:.1f}ê°œ")
else:
    print("âš ï¸  í‰ê·  ê³„ì‚°ì„ ìœ„í•œ ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ìµœì¢… ì €ì¥
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, indent=2, ensure_ascii=False)

print(f"Saved final output to {output_path}")
