#!/usr/bin/env python3
from __future__ import annotations

import re
import string
import sys
from pathlib import Path
from typing import Dict
import spacy
import inflect

import networkx as nx

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í…ìŠ¤íŠ¸ ì •ê·œí™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë¡œë“œ
nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])
inflector = inflect.engine()

def normalize_text(text: str) -> str:
    text = text.lower()

    # â‘  ê´„í˜¸ í›„ì¹˜ ì œê±° â†’ e.g., "battery (chemistry)" â†’ "battery"
    text = re.sub(r"\s*\(.*?\)$", "", text)

    # â‘¡ í•˜ì´í”ˆ ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ì •ë¦¬ â†’ e.g., "non-smoker" â†’ "smoker"
    text = re.sub(r"^(ex|pre|non)-", "", text)
    text = re.sub(r"-(like|type|based)$", "", text)

    # â‘¢ íŠ¹ìˆ˜ë¬¸ì ì œê±° â†’ e.g., "battery!" â†’ "battery"
    text = re.sub(r"[^a-z0-9\s]", "", text)

    # â‘£ ê³µë°± ì •ë¦¬
    text = " ".join(text.split())

    # â‘¤ spaCy ê¸°ë°˜ ì–´ê·¼í™” (lemmatization)
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc if not token.is_space]

    # â‘¥ ë³µìˆ˜í˜• ì²˜ë¦¬ â†’ e.g., "companies" â†’ "company"
    singulars = [inflector.singular_noun(w) or w for w in lemmas]

    # â‘¦ ìµœì¢… ì •ë¦¬
    return " ".join(singulars).strip()

def snake(s: str) -> str:
    return re.sub(r"[^a-z0-9]+", "_", s.lower()).strip("_")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# êµ¬ë¶„ì íŒ¨í„´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DELIM_REGEX = re.compile(
    r"\s*/\s*"          # slash
    r"|\s+and\s+"       # and
    r"|\s*&\s*"         # &
    r"|\s*,\s*"         # comma
    r"|\s*,\s*and\s+",  # , and
    flags=re.I,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³´ì¡° ì¸ë±ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_norm_label_index(G: nx.Graph) -> Dict[str, str]:
    """ì •ê·œí™” ë¼ë²¨ â†’ ë…¸ë“œ ID"""
    idx: Dict[str, str] = {}
    for nid, data in G.nodes(data=True):
        if data.get("type") == "entity":
            continue
        norm = normalize_text(data.get("label", ""))
        if norm and norm not in idx:
            idx[norm] = nid
    return idx

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ë³€í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_composite_subtopics(G: nx.Graph) -> None:
    label2nid = build_norm_label_index(G)

    # ëŒ€ìƒ ë…¸ë“œ ìˆ˜ì§‘
    targets = [
        (nid, data["label"])
        for nid, data in G.nodes(data=True)
        if data.get("type") != "entity" # ì—”í‹°í‹° ì œì™¸
        and DELIM_REGEX.search(str(data.get("label", "")))
    ]

    for orig_nid, raw_label in targets:
        parts = [p.strip() for p in DELIM_REGEX.split(raw_label) if p.strip()]
        if len(parts) < 2:
            continue

        incident_edges = list(G.edges(orig_nid, data=True))

        for part in parts:
            norm = normalize_text(part)
            if not norm:
                continue

            # 1) ê¸°ì¡´ ë…¸ë“œ ì¬ì‚¬ìš© or ì‹ ê·œ ìƒì„±
            if norm in label2nid:
                dest = label2nid[norm]
            else:
                dest = f"subtopic_{snake(norm)}"
                if dest not in G:
                    attrs = G.nodes[orig_nid].copy()
                    attrs["label"] = part          # ì›ë³¸ ê·¸ëŒ€ë¡œ í‘œì‹œ
                    G.add_node(dest, **attrs)
                label2nid[norm] = dest

            # 2) ì—£ì§€ ë³µì‚¬ (ì¤‘ë³µ ë°©ì§€)
            for u, v, edata in incident_edges:
                nbr = v if u == orig_nid else u
                if not G.has_edge(dest, nbr):
                    G.add_edge(dest, nbr, **edata)

        # 3) ë³µí•© ë…¸ë“œ ì‚­ì œ
        G.remove_node(orig_nid)

        for i, (u, v, data) in enumerate(G.edges(data=True)):
            data['id'] = str(i)

# input/output ê²½ë¡œ ì§ì ‘ ì§€ì • (ë˜ëŠ” sys.argv ì‚¬ìš© ìœ ì§€ ê°€ëŠ¥)
src = Path("hotpotQA/graph_v1.gexf")
dst = Path("hotpotQA/graph_v1_processed.gexf")

if not src.exists():
    print(f"âŒ  input file not found: {src}")
    sys.exit(1)

print(f"ğŸ“–  loading graph: {src}")
graph = nx.read_gexf(src)

normalize_composite_subtopics(graph)

print(f"ğŸ’¾  writing graph â†’ {dst}")
nx.write_gexf(graph, dst)
print("âœ…  done.")