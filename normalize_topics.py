#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
normalize_topics.py Â· topic / subtopic node cleanup
---------------------------------------------------
* compositeâ€¯split â†’ labelâ€¯normalize (lemma + singular)
* filler stripping (role ofâ€¦, impact ofâ€¦)
* headâ€‘noun í´ëŸ¬ìŠ¤í„°ë¡œ í‘œì¤€ subtopic ID ë¶€ì—¬ (IDÂ ì •ë ¬ìš©)
* **headâ€‘only subtopic ë¶„í•´**
  Â· ì—°ê²°ëœ ê° topic ì— ëŒ€í•´ `<topic> <head>` ì„œë¸Œí† í”½ìœ¼ë¡œ ëª¨ë“  ì—£ì§€â€¯ì´ê´€
  Â· entityâ€‘subtopic ì—£ì§€ë„ ë™ì¼í•˜ê²Œ ì¬ë°°ì„ 
  Â· ì´ê´€ì´ í•˜ë‚˜ë„ ì„±ê³µí•˜ì§€ ëª»í•œ headâ€‘only ë…¸ë“œëŠ” ê·¸ëŒ€ë¡œ ë‘  (ì •ë³´ ì†ì‹¤ ë°©ì§€)
  Â· ëª¨ë“  ì—£ì§€ë¥¼ ì˜®ê¸°ê³  degree==0 ì´ë©´ ë…¸ë“œ ì œê±°
* edge ì†ì„±(label / sentence / weight) ì•ˆì „íˆ í•©ì¹¨

â€£ I/O ê²½ë¡œëŠ” í•˜ë“œì½”ë”©(ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ ìƒìˆ˜)
"""

from __future__ import annotations
import re, csv, collections, sys
from pathlib import Path

import networkx as nx
import spacy, inflect

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ› ï¸  ê²½ë¡œ í•˜ë“œì½”ë”© â€“â€†í•„ìš” ì‹œë§Œ ìˆ˜ì •!
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INPUT_GEXF  = Path("hotpotQA/graph_v1.gexf")
OUTPUT_GEXF = Path("hotpotQA/normalized_graph_v1.gexf")
SUB_CSV     = Path("hotpotQA/hotpot.csv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# resources & ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
nlp        = spacy.load("en_core_web_sm", disable=["ner", "parser"])
inflector  = inflect.engine()
MIN_SIZE   = 1   # headâ€‘noun í´ëŸ¬ìŠ¤í„° ìµœì†Œ í¬ê¸°

FILLER_PAT = [
    r"^(type|kind|form|class|classification|list|number|count|history|study|future|status|role|impact|effect|origin|case study) of ",
    r" (in|for|at|within|by|about|among|between|under) .*$",
]

DELIM_RE = re.compile(r"\s*/\s*|\s+and\s+|\s*&\s*|\s*,\s*|\s*,\s*and\s+", re.I)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# helper í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def normalize_text(txt: str) -> str:
    txt = txt.lower().replace("_", " ")
    txt = re.sub(r"\s*\(.*?\)$", "", txt)
    txt = re.sub(r"[^a-z0-9\s\-]", " ", txt)
    txt = " ".join(txt.split())
    lemmas = [t.lemma_ for t in nlp(txt) if not t.is_space]
    singulars = [inflector.singular_noun(w) or w for w in lemmas]
    return " ".join(singulars).strip()

def strip_fillers(txt: str) -> str:
    for pat in FILLER_PAT:
        txt = re.sub(pat, "", txt)
    return txt.strip()

def head_noun(label: str) -> str:
    toks = label.split()
    for w in reversed(toks):
        if len(w) > 3 and w not in {"of", "for", "and", "the", "in"}:
            return w
    return toks[-1]

def snake(s: str) -> str:
    return re.sub(r"[^a-z0-9]+", "_", s.lower()).strip("_")

def merge_edge_attrs(dst: dict, src: dict, sep: str = " / ") -> None:
    for k in ("label", "sentence"):
        if src.get(k):
            if not dst.get(k):
                dst[k] = src[k]
            elif src[k] not in dst[k]:
                dst[k] += sep + src[k]
    dst["weight"] = dst.get("weight", 1) + src.get("weight", 1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) headâ€‘noun í´ëŸ¬ìŠ¤í„° ë§¤í•‘ (ID í‘œì¤€í™”ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_cluster_map(sub_csv: Path) -> dict[str, str]:
    subs_raw = [r[0] for r in csv.reader(sub_csv.open(encoding="utf-8-sig")) if r]
    clusters: dict[str, list[str]] = collections.defaultdict(list)
    for s in subs_raw:
        clusters[head_noun(strip_fillers(normalize_text(s)))].append(s)
    mapping: dict[str, str] = {}
    for hn, items in clusters.items():
        canon = f"subtopic_{snake(hn)}"
        for it in items:
            mapping[normalize_text(it)] = canon
    return mapping

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) headâ€‘only ì„œë¸Œí† í”½ ë¶„í•´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def split_head_only_subtopics(G: nx.Graph):
    """Break a headâ€‘only subtopic (e.g. `director`) into `<topic> <head>` variants.
    Entity edges are migrated *only* when their original edge carries the same
    topic attribute, preventing duplicate edges to unrelated topics.  The
    original node is removed when all of its edges are reâ€‘wired.
    """

    sub_nodes = [n for n, d in G.nodes(data=True) if d.get("type") == "subtopic"]
    norm_lbl  = {n: normalize_text(G.nodes[n]["label"]) for n in sub_nodes}
    sub_norm_set = set(norm_lbl.values())  # fast existence test

    for nid in sub_nodes:
        head = norm_lbl[nid]
        if " " in head:
            continue  # already has a prefix

        # neighbours
        topic_nodes   = [t for t in G[nid] if G.nodes[t].get("type") == "topic"]
        entity_nodes  = [e for e in G[nid] if G.nodes[e].get("type") == "entity"]
        if not topic_nodes or not entity_nodes:
            continue

        moved_any = False
        moved_entities : set[str] = set()  # track moved entities

        for t in topic_nodes:
            topic_norm = normalize_text(G.nodes[t]["label"])  # e.g. "film"
            cand_label = f"{topic_norm} {head}"
            if cand_label not in sub_norm_set:
                continue  # the `${topic} ${head}` subtopic does not exist
            cand_id = f"subtopic_{snake(cand_label)}"

            # topic â†” candidate edge
            if not G.has_edge(cand_id, t):
                G.add_edge(cand_id, t, label="has_topic", relation_type="topic_relation")

            # entity â†” candidate edges (topic attr check)
            for e in entity_nodes:
                edge_topic = normalize_text(G[nid][e].get("topic", ""))
                if edge_topic and edge_topic != topic_norm:
                    continue  # entity belongs to different topic
                if not G.has_edge(cand_id, e):
                    new_attrs = G[nid][e].copy()
                    new_attrs["topic"] = topic_norm
                    G.add_edge(cand_id, e, **new_attrs)
                else:
                    merge_edge_attrs(G[cand_id][e], G[nid][e])
                moved_entities.add(e)
            moved_any = True

        # delete original edges / node only if something was moved
        # ì˜®ê²¨ ë†“ì€ ì—£ì§€ë“¤ë§Œ ì‚­ì œ
        if moved_any:
            for e in moved_entities:
                if G.has_edge(nid, e):
                    G.remove_edge(nid, e)
            # topic ì—£ì§€ëŠ” ëª¨ë‘ ì˜®ê²¼ìœ¼ë¯€ë¡œ ì•ˆì „íˆ ì‚­ì œ
            for t in topic_nodes:
                if G.has_edge(nid, t):
                    G.remove_edge(nid, t)

            # ì•„ì§ ë‚¨ì•„ ìˆëŠ” ì—”í‹°í‹°â€‘ì—£ì§€ê°€ ì—†ìœ¼ë©´ ë…¸ë“œ ì œê±°
            if G.degree(nid) == 0:
                G.remove_node(nid)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê·¸ë˜í”„ ì •ê·œí™” ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def normalize_graph(src: Path, dst: Path, sub_csv: Path):
    if not src.exists():
        sys.exit("âŒ input not found: {}".format(src))
    if not src.exists():
        sys.exit(f"âŒ input not found: {src}")
    print("ğŸ“– loading graph â€¦")
    G = nx.read_gexf(src)

    # A) ID í‘œì¤€í™” ë§¤í•‘
    mapping = build_cluster_map(sub_csv)
    print(f"ğŸ—‚ï¸  cluster map size: {len(mapping):,}")

    # B) composite split
    for nid, data in list(G.nodes(data=True)):
        if data.get("type") == "entity":
            continue
        label = str(data.get("label", ""))
        if not DELIM_RE.search(label):
            continue
        parts = [p.strip() for p in DELIM_RE.split(label) if p.strip()]
        if len(parts) < 2:
            continue
        inc_edges = list(G.edges(nid, data=True))
        for part in parts:
            norm = normalize_text(part)
            if not norm:
                continue
            prefix = "topic" if data.get("type") == "topic" else "subtopic"
            new_id = f"{prefix}_{snake(norm)}"
            if new_id not in G:
                attrs = data.copy()
                attrs["label_raw"], attrs["label"] = part, norm
                G.add_node(new_id, **attrs)
            for u, v, edata in inc_edges:
                other = v if u == nid else u
                if not G.has_edge(new_id, other):
                    G.add_edge(new_id, other, **edata)
        G.remove_node(nid)

    # C) label ì •ê·œí™” & ë³‘í•©
    for nid in list(G.nodes):
        d = G.nodes[nid]
        ntype = d.get("type")
        if ntype not in {"topic", "subtopic"}:
            continue
        raw = str(d.get("label", "")) or str(nid)
        norm = normalize_text(raw)
        d["label_raw"], d["label"] = raw, norm
        target_id = f"{ntype}_{snake(norm)}"
        if ntype == "subtopic" and norm in mapping:
            target_id = mapping[norm]
        if target_id == nid:
            continue
        if target_id in G:
            # ì†ì„± ë³‘í•© + ì—£ì§€ ì¬ë°°ì„ 
            for nbr, edata in list(G[nid].items()):
                if nbr == target_id:
                    continue
                if G.has_edge(target_id, nbr):
                    merge_edge_attrs(G[target_id][nbr], edata)
                else:
                    G.add_edge(target_id, nbr, **edata)
            G.remove_node(nid)
        else:
            nx.relabel_nodes(G, {nid: target_id}, copy=False)

    # D) headâ€‘only subtopic ë¶„í•´
    split_head_only_subtopics(G)

    # E) edge id ì¬ë¶€ì—¬
    for i, (_, _, d) in enumerate(G.edges(data=True)):
        d["id"] = str(i)

    print("ğŸ’¾ writing â†’", dst)
    nx.write_gexf(G, dst)
    print("âœ… done")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    normalize_graph(INPUT_GEXF, OUTPUT_GEXF, SUB_CSV)
