import json
from pathlib import Path
from typing import List, Dict, Any
import tiktoken
from tqdm import tqdm
from collections.abc import Iterable

# â”€â”€â”€â”€â”€ ì‚¬ìš©ì ì„¤ì • â”€â”€â”€â”€â”€
TXT_PATH   = "hotpotQA/contexts_distractor_1000.txt"
GRAPH_PATH = "hotpotQA/graph_v1.json"
OUTPUT_PATH = "hotpotQA/graph_with_chunks.json"

MODEL_NAME = "gpt-4o-mini"
MAX_TOKENS = 1200
OVERLAP    = 100
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chunk_text(text: str, max_tokens: int, overlap: int, model: str) -> List[str]:
    enc = tiktoken.encoding_for_model(model)
    tokens = enc.encode(text)
    chunks, start = [], 0
    while start < len(tokens):
        chunk_tokens = tokens[start : start + max_tokens]
        chunks.append(enc.decode(chunk_tokens))
        start += max_tokens - overlap
    return chunks

def normalize(s: str) -> str:
    return " ".join(s.replace("\n", " ").split())

def find_sentence_chunks(sentence: str, chunks: List[str]) -> List[int]:
    target = normalize(sentence)
    return [i for i, ch in enumerate(chunks) if target in normalize(ch)]

def flatten_until_dict(seq: Iterable) -> List[Dict[str, Any]]:
    """ë¦¬ìŠ¤íŠ¸ ì¤‘ì²©ì„ dict ë ˆë²¨ê¹Œì§€ ì „ë¶€ í‰íƒ„í™”"""
    flat: List[Dict[str, Any]] = []
    stack = list(seq)
    while stack:
        cur = stack.pop()
        if isinstance(cur, dict):
            flat.append(cur)
        elif isinstance(cur, list):
            stack.extend(cur)        # ë” í’€ì–´ì•¼ í•¨ â†’ ìŠ¤íƒì— ì¶”ê°€
        else:
            # dictë„ listë„ ì•„ë‹ˆë©´ ë¬´ì‹œ(ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…)
            pass
    return flat[::-1]                # ì›ë˜ ìˆœì„œ ìœ ì§€

# 1) í…ìŠ¤íŠ¸ â†’ ì²­í¬
full_text = Path(TXT_PATH).read_text(encoding="utf-8")
chunks = chunk_text(full_text, MAX_TOKENS, OVERLAP, MODEL_NAME)
print(f"âœ… ì²­í¬ {len(chunks)}ê°œ ìƒì„±ë¨")

# 2) ê·¸ë˜í”„ ë¡œë“œ ë° í‰íƒ„í™”
with open(GRAPH_PATH, encoding="utf-8") as f:
    raw_graph = json.load(f)

graph = flatten_until_dict(raw_graph)
print(f"âœ… í‰íƒ„í™” í›„ ê·¸ë˜í”„ í•­ëª© {len(graph)}ê°œ")

# 3) ë¬¸ì¥ â†” ì²­í¬ ë§¤ì¹­
for item in tqdm(graph, desc="ğŸ“Œ ë§¤ì¹­ ì¤‘"):
    sent = item.get("sentence", "")
    item["chunk_ids"] = find_sentence_chunks(sent, chunks)
    if not item["chunk_ids"]:
        tqdm.write(f"âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨: {sent[:70]}...")

# 4) ì €ì¥
with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    json.dump(graph, f, indent=2, ensure_ascii=False)
print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_PATH}")
