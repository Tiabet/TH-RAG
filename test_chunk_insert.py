# import json
# from pathlib import Path
# from typing import List, Dict, Any
# import tiktoken
# from tqdm import tqdm
# from collections.abc import Iterable

# # â”€â”€â”€â”€â”€ ì‚¬ìš©ì ì„¤ì • â”€â”€â”€â”€â”€
# TXT_PATH   = "MultihopRAG/contexts.txt"
# GRAPH_PATH = "MultihopRAG/graph_v1.json"
# OUTPUT_PATH = "MultihopRAG/graph_with_chunks.json"

# MODEL_NAME = "gpt-4o-mini"
# MAX_TOKENS = 1200
# OVERLAP    = 100
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# def chunk_text(text: str, max_tokens: int, overlap: int, model: str) -> List[str]:
#     enc = tiktoken.encoding_for_model(model)
#     tokens = enc.encode(text)
#     chunks, start = [], 0
#     while start < len(tokens):
#         chunk_tokens = tokens[start : start + max_tokens]
#         chunks.append(enc.decode(chunk_tokens))
#         start += max_tokens - overlap
#     return chunks

# def normalize(s: str) -> str:
#     return " ".join(s.replace("\n", " ").split())

# def find_sentence_chunks(sentence: str, chunks: List[str]) -> List[int]:
#     target = normalize(sentence)
#     return [i for i, ch in enumerate(chunks) if target in normalize(ch)]

# def flatten_until_dict(seq: Iterable) -> List[Dict[str, Any]]:
#     """ë¦¬ìŠ¤íŠ¸ ì¤‘ì²©ì„ dict ë ˆë²¨ê¹Œì§€ ì „ë¶€ í‰íƒ„í™”"""
#     flat: List[Dict[str, Any]] = []
#     stack = list(seq)
#     while stack:
#         cur = stack.pop()
#         if isinstance(cur, dict):
#             flat.append(cur)
#         elif isinstance(cur, list):
#             stack.extend(cur)        # ë” í’€ì–´ì•¼ í•¨ â†’ ìŠ¤íƒì— ì¶”ê°€
#         else:
#             # dictë„ listë„ ì•„ë‹ˆë©´ ë¬´ì‹œ(ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…)
#             pass
#     return flat[::-1]                # ì›ë˜ ìˆœì„œ ìœ ì§€

# # 1) í…ìŠ¤íŠ¸ â†’ ì²­í¬
# full_text = Path(TXT_PATH).read_text(encoding="utf-8")
# chunks = chunk_text(full_text, MAX_TOKENS, OVERLAP, MODEL_NAME)
# print(f"âœ… ì²­í¬ {len(chunks)}ê°œ ìƒì„±ë¨")

# # 2) ê·¸ë˜í”„ ë¡œë“œ ë° í‰íƒ„í™”
# with open(GRAPH_PATH, encoding="utf-8") as f:
#     raw_graph = json.load(f)

# graph = flatten_until_dict(raw_graph)
# print(f"âœ… í‰íƒ„í™” í›„ ê·¸ë˜í”„ í•­ëª© {len(graph)}ê°œ")

# # 3) ë¬¸ì¥ â†” ì²­í¬ ë§¤ì¹­
# for item in tqdm(graph, desc="ğŸ“Œ ë§¤ì¹­ ì¤‘"):
#     sent = item.get("sentence", "")
#     item["chunk_ids"] = find_sentence_chunks(sent, chunks)
#     if not item["chunk_ids"]:
#         tqdm.write(f"âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨: {sent[:70]}...")

# # 4) ì €ì¥
# with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
#     json.dump(graph, f, indent=2, ensure_ascii=False)
# print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_PATH}")

import json

# ì˜ˆì‹œ: JSON ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë¶ˆëŸ¬ì˜¨ ê²½ìš° (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŒ)
with open('MultihopRAG\graph_v1.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# chunk_id ì¶”ê°€
for chunk_id, chunk in enumerate(data):
    for item in chunk:
        item["chunk_id"] = chunk_id

# ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)
with open('dataset_with_chunk_id.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, indent=2, ensure_ascii=False)

# í™•ì¸ ì¶œë ¥ (ì„ íƒì‚¬í•­)
print(json.dumps(data[:1], indent=2, ensure_ascii=False))  # ì²« ë²ˆì§¸ ì²­í¬ë§Œ í™•ì¸
